step 1:
# Small EC2 Instance Airflow Tutorial

## EC2 Configuration
- **Instance Type**: T2.small
- **Key Pair**: Create a new key pair
- **Image**: Ubuntu

## Connect to EC2 Instance
After launching your EC2 instance, connect to it in aws


## Commands to Run
step 2:
 sudo apt update
step 3:
Install Python 3 and pip:

sudo apt install python3-pip
step 4:
Install SQLite3:

sudo apt install sqlite3
step 5:
Install the Python 3.10 virtual environment package:

sudo apt install python3.10-venv
step 6:
Create a Python virtual environment:

python3 -m venv venv
step 7:
Activate the virtual environment:

source venv/bin/activate
step 8:
(Optional) Install PostgreSQL development libraries:

sudo apt-get install libpq-dev
step 9:
Install Apache Airflow with PostgreSQL support:

pip install "apache-airflow[postgres]==2.5.0" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.5.0/constraints-3.7.txt"
step 10:
Initialize the Airflow database:

airflow db init
step 11:
Install PostgreSQL and its contrib package:

sudo apt-get install postgresql postgresql-contrib
step 12:
Switch to the PostgreSQL user:

sudo -i -u postgres
step 13:
Access the PostgreSQL shell:

psql
step 14:
Create the Airflow database and user:

CREATE DATABASE airflow;
CREATE USER airflow WITH PASSWORD 'airflow';
GRANT ALL PRIVILEGES ON DATABASE airflow TO airflow;
Exit the PostgreSQL shell by pressing Ctrl + D.
Give ls it display version of posytgres
Exit from prostgres press Ctrl + D. 
Give ls it display airflow env
step 15:
Navigate to the Airflow directory:

cd airflow
step 16:
Replace the connection string in the airflow.cfg file to use PostgreSQL instead of SQLite:

sed -i 's#sqlite:////home/ubuntu/airflow/airflow.db#postgresql+psycopg2://airflow:airflow@localhost/airflow#g' airflow.cfg
step 17:
Verify the SQL Alchemy connection string:

grep sql_alchemy airflow.cfg
step 18:
Check the executor configuration:

grep executor airflow.cfg
step 19:
Replace SequentialExecutor with LocalExecutor:

sed -i 's#SequentialExecutor#LocalExecutor#g' airflow.cfg
step 20:
Re-initialize the Airflow database:

airflow db init
step 21:
Create an Airflow user:

airflow users create -u airflow -f airflow -l airflow -r Admin -e airflow@gmail.com
Enter Password: airflow
Repeat Password: airflow
step 22:
Update security group inbound rules in your EC2 instance:

Type: Custom TCP
Port Range: 8080
Source: Anywhere (IPV4)
Save the rules.
step 23:
Start the Airflow webserver:

airflow webserver &
step 24:
Start the Airflow scheduler:

airflow scheduler
step 25:
Copy the public IPv4 DNS of your EC2 instance and open it in your browser with port 8080:

http://your-ec2-public-ip:8080
step 26:
Use below dag code as a test. make sure to mkdir dag folder and then put this .py script in there!

from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
    'retries': 1,
}

dag = DAG(
    'my_new_dag',
    default_args=default_args,
    schedule_interval='@daily',
)

start = DummyOperator(task_id='start', dag=dag)
end = DummyOperator(task_id='end', dag=dag)

start >> end
